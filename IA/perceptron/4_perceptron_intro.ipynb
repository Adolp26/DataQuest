{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução à Inteligência Artificial e Redes Neurais com Implementação de Perceptron\n",
    "\n",
    "## Sumário\n",
    "1. [Introdução à Inteligência Artificial](#introdução-à-inteligência-artificial)\n",
    "2. [Perceptron](#perceptron)\n",
    "   - [Implementação de um Perceptron](#implementação-de-um-perceptron)\n",
    "   - [Exemplo de Treinamento com Perceptron](#exemplo-de-treinamento-com-perceptron)\n",
    "3. [Redes Neurais e MLP](#redes-neurais-e-mlp)\n",
    "   - [Topologia de Redes Neurais](#topologia-de-redes-neurais)\n",
    "   - [Implementação de uma MLP](#implementação-de-uma-mlp)\n",
    "   - [Exemplo de Treinamento com MLP](#exemplo-de-treinamento-com-mlp)\n",
    "4. [Conclusão](#conclusão)\n",
    "\n",
    "---\n",
    "\n",
    "A Inteligência Artificial (IA) é um campo da ciência da computação que visa criar sistemas capazes de realizar tarefas que normalmente exigiriam inteligência humana, como reconhecimento de padrões, tomada de decisões e aprendizado. Um dos métodos mais populares de IA é o aprendizado de máquina, que utiliza dados para \"ensinar\" um modelo a resolver problemas.\n",
    "\n",
    "As redes neurais, inspiradas no funcionamento do cérebro humano, são estruturas fundamentais no aprendizado de máquina. Elas consistem em unidades (neurônios) interconectadas e organizadas em camadas.\n",
    "\n",
    "---\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "O Perceptron é o tipo mais básico de neurônio artificial, que recebe múltiplas entradas, calcula uma soma ponderada, aplica uma função de ativação (normalmente uma função degrau) e gera uma saída. Um perceptron pode ser usado para resolver problemas de classificação binária simples, como classificar pontos de dados em uma linha.\n",
    "\n",
    "### Implementação de um Perceptron\n",
    "\n",
    "Abaixo, vamos implementar um perceptron simples em Python. Esse perceptron pode ser treinado para aprender a classificar dados com base em dois valores de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\work\\ides\\python\\311\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\work\\ides\\python\\311\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        self.weights = np.zeros(input_size + 1)  # Inclui o peso para o bias\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  # Adiciona o bias\n",
    "        activation = np.dot(self.weights, x)\n",
    "        return 1 if activation >= 0 else 0\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for xi, target in zip(X, y):\n",
    "                prediction = self.predict(xi)\n",
    "                error = target - prediction\n",
    "                xi = np.insert(xi, 0, 1)  # Adiciona o bias\n",
    "                self.weights += self.learning_rate * error * xi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de Treinamento com Perceptron\n",
    "Vamos treinar nosso perceptron para resolver um problema básico de classificação de OR lógico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: [0 0 0], Saída prevista: 1\n",
      "Entrada: [0 0 1], Saída prevista: 1\n",
      "Entrada: [0 1 0], Saída prevista: 1\n",
      "Entrada: [0 1 1], Saída prevista: 1\n",
      "Entrada: [1 0 0], Saída prevista: 1\n",
      "Entrada: [1 0 1], Saída prevista: 1\n",
      "Entrada: [1 1 0], Saída prevista: 1\n",
      "Entrada: [1 1 1], Saída prevista: 0\n"
     ]
    }
   ],
   "source": [
    "# Dados de entrada para a operação OR\n",
    "X = np.array([\n",
    "    [0 , 0 , 0],\n",
    "    [0 , 0 , 1],\n",
    "    [0 , 1 , 0],\n",
    "    [0 , 1 , 1],\n",
    "    [1 , 0 , 0],\n",
    "    [1 , 0 , 1],\n",
    "    [1 , 1 , 0],\n",
    "    [1 , 1 , 1]\n",
    "])\n",
    "y = np.array([1, 1, 1, 1, 1, 1, 1, 0])  # Saída desejada para OR\n",
    "\n",
    "# Instancia o perceptron e treina\n",
    "perceptron = Perceptron(input_size=3, learning_rate=0.1)\n",
    "perceptron.train(X, y, epochs=6)\n",
    "\n",
    "# Testa o perceptron\n",
    "for xi in X:\n",
    "    print(f\"Entrada: {xi}, Saída prevista: {perceptron.predict(xi)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neurais e MLP\n",
    "Para problemas mais complexos, uma única camada de perceptron não é suficiente. Nesse caso, precisamos de uma rede neural multicamada (MLP - Multilayer Perceptron), que inclui neurônios organizados em múltiplas camadas e permite que o modelo aprenda representações complexas.\n",
    "\n",
    "### Topologia de Redes Neurais\n",
    "A topologia de uma rede neural refere-se à estrutura e organização das camadas e neurônios. As redes neurais geralmente consistem em:\n",
    "\n",
    "    - Camada de Entrada: onde os dados são recebidos pela rede.\n",
    "    - Camadas Ocultas: onde os dados passam por transformações não-lineares.\n",
    "    - Camada de Saída: onde os resultados finais são obtidos.\n",
    "\n",
    "Cada camada tem neurônios, e cada neurônio em uma camada está conectado aos neurônios da próxima camada.\n",
    "\n",
    "### Implementação de uma MLP\n",
    "Aqui vamos implementar uma MLP com duas camadas: uma camada oculta e uma camada de saída. Utilizaremos a função de ativação Sigmoide para introduzir a não-linearidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        # Pesos para as conexões de entrada-oculta e oculta-saída\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # Função de ativação sigmoide\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        # Derivada da função sigmoide\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagação para frente\n",
    "        self.hidden = self.sigmoid(np.dot(x, self.weights_input_hidden))\n",
    "        self.output = self.sigmoid(np.dot(self.hidden, self.weights_hidden_output))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        # Backpropagation para atualizar pesos\n",
    "        output_error = y - self.output\n",
    "        output_delta = output_error * self.sigmoid_derivative(self.output)\n",
    "        \n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden)\n",
    "        \n",
    "        # Atualização dos pesos\n",
    "        self.weights_hidden_output += self.hidden.T.dot(output_delta) * self.learning_rate\n",
    "        self.weights_input_hidden += x.T.dot(hidden_delta) * self.learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for xi, target in zip(X, y):\n",
    "                self.forward(xi)\n",
    "                self.backward(xi.reshape(1, -1), target.reshape(1, -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de Treinamento com MLP\n",
    "Agora, vamos treinar a nossa MLP para resolver o problema de classificação XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (1,1) not aligned: 2 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Instancia a MLP e treina\u001b[39;00m\n\u001b[0;32m     16\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLP(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Testa a MLP\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m X:\n",
      "Cell \u001b[1;32mIn[24], line 38\u001b[0m, in \u001b[0;36mMLP.train\u001b[1;34m(self, X, y, epochs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xi, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, y):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(xi)\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 31\u001b[0m, in \u001b[0;36mMLP.backward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     28\u001b[0m hidden_delta \u001b[38;5;241m=\u001b[39m hidden_error \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid_derivative(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Atualização dos pesos\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_hidden_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_delta\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_input_hidden \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(hidden_delta) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,) and (1,1) not aligned: 2 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Dados de entrada para a operação XOR\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])\n",
    "\n",
    "# Instancia a MLP e treina\n",
    "mlp = MLP(input_size=2, hidden_size=2, output_size=1, learning_rate=0.1)\n",
    "mlp.train(X, y, epochs=100)\n",
    "\n",
    "# Testa a MLP\n",
    "for xi in X:\n",
    "    print(f\"Entrada: {xi}, Saída prevista: {mlp.forward(xi)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
